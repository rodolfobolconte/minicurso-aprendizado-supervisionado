{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36864bitcefaae51de97494c8b3e896cae0342ea",
   "display_name": "Python 3.6.8 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classificação de Dados com Python 3\n",
    "\n",
    "## 0- Objetivo\n",
    "\n",
    "Carregar Conjunto de Dados com informações de Diagnósticos de Câncer de Mama para realizar a Classificação (em Malígno ou Benígno) destes dados utilizando algoritmos de Aprendizado de Máquina Supervisionado em Python.\n",
    "\n",
    "## 1- Conjunto de Dados (_Dataset_)\n",
    "\n",
    "Conjunto com amostras de Diagnósticos de Câncer de Mama com duas Classificações: Malígno (M) e Benígno (B).\n",
    "\n",
    "Informações do Conjnuto:\n",
    "- Construído em 1995;\n",
    "- Origem: _scikit-learn_;\n",
    "- Estrutura de Dados ao Carregar: _bunch_;\n",
    "- Atributos da Estrutura:\n",
    "    - _target_: rótulos das amostras;\n",
    "    - _target_names_: significado dos rótulos;\n",
    "    - _feature_names_: nome dos atributos;\n",
    "    - _DESCR_: descrição do Conjunto de Dados;\n",
    "    - _filename_: localização do Conjunto de Dados em .CSV.\n",
    "- Página do Conjunto de Dados: http://bit.ly/load_breast_cancer\n",
    "\n",
    "Informações dos Dados:\n",
    "- Número de Diagnósticos (Amostras): 569;\n",
    "- Duas Classificações:\n",
    "    - Diagnósticos Malígnos: 212;\n",
    "    - Diagnósticos Benígnos: 357.\n",
    "- Quantidade de Atributos: 30.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Rótulos das amostras:\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 1 0 0 0 0 0 0 0 0 1 0 1 1 1 1 1 0 0 1 0 0 1 1 1 1 0 1 0 0 1 1 1 1 0 1 0 0\n 1 0 1 0 0 1 1 1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 0 1 1 0 1 1\n 1 1 1 1 1 1 0 0 0 1 0 0 1 1 1 0 0 1 0 1 0 0 1 0 0 1 1 0 1 1 0 1 1 1 1 0 1\n 1 1 1 1 1 1 1 1 0 1 1 1 1 0 0 1 0 1 1 0 0 1 1 0 0 1 1 1 1 0 1 1 0 0 0 1 0\n 1 0 1 1 1 0 1 1 0 0 1 0 0 0 0 1 0 0 0 1 0 1 0 1 1 0 1 0 0 0 0 1 1 0 0 1 1\n 1 0 1 1 1 1 1 0 0 1 1 0 1 1 0 0 1 0 1 1 1 1 0 1 1 1 1 1 0 1 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 0 1 1 1 1 0 0 0 1 1\n 1 1 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0\n 0 1 0 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1\n 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 0 1 1 1 1 1 0 1 1\n 0 1 0 1 1 0 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1\n 1 1 1 1 1 1 0 1 0 1 1 0 1 1 1 1 1 0 0 1 0 1 0 1 1 1 1 1 0 1 1 0 1 0 1 0 0\n 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 0 0 0 0 0 0 1]\n\nClassificações possíveis:\n['malignant' 'benign']\n\nAtributos utilizados:\n['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n 'mean smoothness' 'mean compactness' 'mean concavity'\n 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n 'radius error' 'texture error' 'perimeter error' 'area error'\n 'smoothness error' 'compactness error' 'concavity error'\n 'concave points error' 'symmetry error' 'fractal dimension error'\n 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n 'worst smoothness' 'worst compactness' 'worst concavity'\n 'worst concave points' 'worst symmetry' 'worst fractal dimension']\n\nQuantidade de Amostras com Câncer:\n212\n\nQuantidade de Amostras sem Câncer:\n357\n\nCaminho do arquivo .CSV do Conjunto:\nC:\\Programas\\Python\\Python36\\lib\\site-packages\\sklearn\\datasets\\data\\breast_cancer.csv\n\nDescrição do Conjunto de Dados:\n.. _breast_cancer_dataset:\n\nBreast cancer wisconsin (diagnostic) dataset\n--------------------------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 569\n\n    :Number of Attributes: 30 numeric, predictive attributes and the class\n\n    :Attribute Information:\n        - radius (mean of distances from center to points on the perimeter)\n        - texture (standard deviation of gray-scale values)\n        - perimeter\n        - area\n        - smoothness (local variation in radius lengths)\n        - compactness (perimeter^2 / area - 1.0)\n        - concavity (severity of concave portions of the contour)\n        - concave points (number of concave portions of the contour)\n        - symmetry \n        - fractal dimension (\"coastline approximation\" - 1)\n\n        The mean, standard error, and \"worst\" or largest (mean of the three\n        largest values) of these features were computed for each image,\n        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n        13 is Radius SE, field 23 is Worst Radius.\n\n        - class:\n                - WDBC-Malignant\n                - WDBC-Benign\n\n    :Summary Statistics:\n\n    ===================================== ====== ======\n                                           Min    Max\n    ===================================== ====== ======\n    radius (mean):                        6.981  28.11\n    texture (mean):                       9.71   39.28\n    perimeter (mean):                     43.79  188.5\n    area (mean):                          143.5  2501.0\n    smoothness (mean):                    0.053  0.163\n    compactness (mean):                   0.019  0.345\n    concavity (mean):                     0.0    0.427\n    concave points (mean):                0.0    0.201\n    symmetry (mean):                      0.106  0.304\n    fractal dimension (mean):             0.05   0.097\n    radius (standard error):              0.112  2.873\n    texture (standard error):             0.36   4.885\n    perimeter (standard error):           0.757  21.98\n    area (standard error):                6.802  542.2\n    smoothness (standard error):          0.002  0.031\n    compactness (standard error):         0.002  0.135\n    concavity (standard error):           0.0    0.396\n    concave points (standard error):      0.0    0.053\n    symmetry (standard error):            0.008  0.079\n    fractal dimension (standard error):   0.001  0.03\n    radius (worst):                       7.93   36.04\n    texture (worst):                      12.02  49.54\n    perimeter (worst):                    50.41  251.2\n    area (worst):                         185.2  4254.0\n    smoothness (worst):                   0.071  0.223\n    compactness (worst):                  0.027  1.058\n    concavity (worst):                    0.0    1.252\n    concave points (worst):               0.0    0.291\n    symmetry (worst):                     0.156  0.664\n    fractal dimension (worst):            0.055  0.208\n    ===================================== ====== ======\n\n    :Missing Attribute Values: None\n\n    :Class Distribution: 212 - Malignant, 357 - Benign\n\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n\n    :Donor: Nick Street\n\n    :Date: November, 1995\n\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\nhttps://goo.gl/U2Uwz2\n\nFeatures are computed from a digitized image of a fine needle\naspirate (FNA) of a breast mass.  They describe\ncharacteristics of the cell nuclei present in the image.\n\nSeparating plane described above was obtained using\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\nConstruction Via Linear Programming.\" Proceedings of the 4th\nMidwest Artificial Intelligence and Cognitive Science Society,\npp. 97-101, 1992], a classification method which uses linear\nprogramming to construct a decision tree.  Relevant features\nwere selected using an exhaustive search in the space of 1-4\nfeatures and 1-3 separating planes.\n\nThe actual linear program used to obtain the separating plane\nin the 3-dimensional space is that described in:\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\nProgramming Discrimination of Two Linearly Inseparable Sets\",\nOptimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\n\nftp ftp.cs.wisc.edu\ncd math-prog/cpo-dataset/machine-learn/WDBC/\n\n.. topic:: References\n\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n     San Jose, CA, 1993.\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n     July-August 1995.\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n     163-171.\n\n"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "\n",
    "print(\"Rótulos das amostras:\\n{}\\n\".format(dataset.target))\n",
    "\n",
    "print(\"Classificações possíveis:\\n{}\\n\".format(dataset.target_names))\n",
    "\n",
    "print('Atributos utilizados:\\n{}\\n'.format(dataset.feature_names))\n",
    "\n",
    "print('Quantidade de Amostras com Câncer:\\n{}\\n'.format(len(dataset.target[dataset.target == 0])))\n",
    "\n",
    "print('Quantidade de Amostras sem Câncer:\\n{}\\n'.format(len(dataset.target[dataset.target == 1])))\n",
    "\n",
    "print('Caminho do arquivo .CSV do Conjunto:\\n{}\\n'.format(dataset.filename))\n",
    "\n",
    "print('Descrição do Conjunto de Dados:\\n{}\\n'.format(dataset.DESCR))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.1- Organizando o Conjunto de Dados\n",
    "\n",
    "Utilizar o método _Holdout_ para dividir o Conjunto de Dados em:\n",
    "\n",
    "- Quantidade de Amostras para Treino: 66% do Conjunto = 376 Amostras;\n",
    "- Quantidade de Amostras para Teste: 34% do Conjunto = 193 Amostras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#transforma os numpy arrays em arrays python\n",
    "samples = np.ndarray.tolist(dataset.data)\n",
    "feature_names = np.ndarray.tolist(dataset.feature_names)\n",
    "\n",
    "#insere a coluna 'target' no array de atributos\n",
    "feature_names.append('target')\n",
    "\n",
    "#insere o array de atributos no início do array de amostras\n",
    "samples.insert(0, feature_names)\n",
    "\n",
    "#variavel com os rótulos das amostras\n",
    "targets = np.ndarray.tolist(dataset.target)\n",
    "\n",
    "#coloca o rótulo de cada amostra em seu próprio array no array de amostras\n",
    "for sample, target in zip(samples[1:], targets):\n",
    "    sample.append(target)\n",
    "\n",
    "#transforma o array python com as amostras em um numpy array para ser transformado em dataframe pandas, por ser melhor estruturado\n",
    "samples = np.asarray(samples)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#transforma o numpy array com as amostras em dataframe pandas\n",
    "samples = pd.DataFrame(samples[1:], index=samples[1:], columns=samples[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1.2- Dividindo o Conjunto de Dados\n",
    "\n",
    "Utilizar o método _Holdout_ para dividir o Conjunto de Dados em:\n",
    "\n",
    "- Quantidade de Amostras para Treino: 66% do Conjunto = 376 Amostras;\n",
    "- Quantidade de Amostras para Teste: 34% do Conjunto = 193 Amostras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Quantidade de Dados para Treino: 376\nQuantidade de Dados para Teste: 193\n"
    }
   ],
   "source": [
    "#reserva os primeiros 376 dados para treino\n",
    "trainX = samples[:376][feature_names[:-1]]\n",
    "trainY = samples[:376][feature_names[-1]].astype(np.int)\n",
    "\n",
    "#reserva os últimos 193 dados para teste\n",
    "testX = samples[376:][feature_names[:-1]]\n",
    "testY = samples[376:][feature_names[-1]].astype(np.int)\n",
    "\n",
    "print('Quantidade de Dados para Treino: {}'.format(len(trainX)))\n",
    "print('Quantidade de Dados para Teste: {}'.format(len(testX)))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2- Algoritmos de Classificação\n",
    "\n",
    "### 2.1- Algoritmos Únicos\n",
    "\n",
    "Utilização de modelos de Algoritmos Únicos para a Classificação de Amostras:\n",
    "- _Multinomial Naive Bayes_;\n",
    "- _Decision Tree_ (Árvore de Decisão)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\nDecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=None, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='best')\n"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#criando os modelos únicos de classificação\n",
    "modelMNB = MultinomialNB()\n",
    "modelDT = DecisionTreeClassifier()\n",
    "\n",
    "print(modelMNB)\n",
    "print(modelDT)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.2- _Ensemble Methods_ (Algoritmos de Conjunto)\n",
    "\n",
    "_Ensemble Methods_ são algoritmos que criam várias instâncias de um único algoritmo de classificação para mediar o resultado para as amostras.\n",
    "\n",
    "Utilização dos _Ensemble Methods_:\n",
    "- _Random Forest_ (Floresta Aleatória);\n",
    "- _Adaptive Boosting_ (Reforço Adaptativo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=None, oob_score=False, random_state=None,\n                       verbose=0, warm_start=False)\nAdaBoostClassifier(algorithm='SAMME.R', base_estimator=None, learning_rate=1.0,\n                   n_estimators=50, random_state=None)\n"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "\n",
    "#criando os modelos ensemble methods de classificação\n",
    "modelRF = RandomForestClassifier()\n",
    "modelAB = AdaBoostClassifier()\n",
    "\n",
    "print(modelRF)\n",
    "print(modelAB)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2.3- Treino e Teste dos Algoritmos\n",
    "\n",
    "Primeiro treinar os Algoritmos com os dados de treino (_trainX_ e _trainY_)\n",
    "\n",
    "Segundo testar os Algoritmos com os dados de teste (_testX_, não precisa dos rótulos das amostras em _testY_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#treinando os modelos com as amostras de treino e seus rótulos\n",
    "modelMNB.fit(trainX, trainY)\n",
    "modelDT.fit(trainX, trainY)\n",
    "modelRF.fit(trainX, trainY)\n",
    "modelAB.fit(trainX, trainY)\n",
    "\n",
    "#testando os modelos com os dados de teste (previsão dos dados)\n",
    "previsionMNB = modelMNB.predict(testX).astype(np.int)\n",
    "previsionDT = modelDT.predict(testX).astype(np.int)\n",
    "previsionRF = modelRF.predict(testX).astype(np.int)\n",
    "previsionAB = modelAB.predict(testX).astype(np.int)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3- Resultados\n",
    "\n",
    "Diversas são as formas de comparar os resultados obtidos nas Previsões dos modelos, sendo comum a utilização da Matriz de Confusão que calcula os 4 resultados possíveis:\n",
    "- Verdadeiro Negativo (VN) - `confusion_matrix[0][0]`;\n",
    "- Falso Positivo (FP) - `confusion_matrix[0][1]`;\n",
    "- Falso Negativo (FN) - `confusion_matrix[1][0]`;\n",
    "- Verdadeiro Positivo (VP) - `confusion_matrix[1][1]`.\n",
    "\n",
    "\n",
    "Os 4 resultados podem ser utilizados em diversas métricas estatísticas, das quais serão utilizadas:\n",
    "- Acurácia (_Accuracy_);\n",
    "- Precisão (_Precision_);\n",
    "- Sensibilidade (_Recall_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Resultados Métricos:\n\nMultinomial Naive Bayes:\n\tMatriz de Confusão - VN:33, FP:11, FN:3, VP:146\n\tAcurácia - 0.9275\n\tPrecisão - 0.9299\n\tSensibilidade - 0.9799\n\nDecision Tree:\n\tMatriz de Confusão - VN:43, FP:1, FN:22, VP:127\n\tAcurácia - 0.8808\n\tPrecisão - 0.9922\n\tSensibilidade - 0.8523\n\nFloresta Aleatória:\n\tMatriz de Confusão - VN:43, FP:1, FN:3, VP:146\n\tAcurácia - 0.9793\n\tPrecisão - 0.9932\n\tSensibilidade - 0.9799\n\nAumento Adaptativo:\n\tMatriz de Confusão - VN:43, FP:1, FN:5, VP:144\n\tAcurácia - 0.9689\n\tPrecisão - 0.9931\n\tSensibilidade - 0.9664\n"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "\n",
    "metricsMNB = {} ; metricsDT = {} ; metricsRF = {} ; metricsAB = {}\n",
    "\n",
    "metricsMNB['confusion'] = confusion_matrix(testY, previsionMNB)\n",
    "metricsMNB['accuracy'] = accuracy_score(testY, previsionMNB)\n",
    "metricsMNB['precision'] = precision_score(testY, previsionMNB)\n",
    "metricsMNB['recall'] = recall_score(testY, previsionMNB)\n",
    "\n",
    "metricsDT['confusion'] = confusion_matrix(testY, previsionDT)\n",
    "metricsDT['accuracy'] = accuracy_score(testY, previsionDT)\n",
    "metricsDT['precision'] = precision_score(testY, previsionDT)\n",
    "metricsDT['recall'] = recall_score(testY, previsionDT)\n",
    "\n",
    "metricsRF['confusion'] = confusion_matrix(testY, previsionRF)\n",
    "metricsRF['accuracy'] = accuracy_score(testY, previsionRF)\n",
    "metricsRF['precision'] = precision_score(testY, previsionRF)\n",
    "metricsRF['recall'] = recall_score(testY, previsionRF)\n",
    "\n",
    "metricsAB['confusion'] = confusion_matrix(testY, previsionAB)\n",
    "metricsAB['accuracy'] = accuracy_score(testY, previsionAB)\n",
    "metricsAB['precision'] = precision_score(testY, previsionAB)\n",
    "metricsAB['recall'] = recall_score(testY, previsionAB)\n",
    "\n",
    "print('Resultados Métricos:')\n",
    "\n",
    "print('\\nMultinomial Naive Bayes:')\n",
    "print('\\tMatriz de Confusão - VN:{}, FP:{}, FN:{}, VP:{}'.format(metricsMNB['confusion'][0][0], metricsMNB['confusion'][0][1], metricsMNB['confusion'][1][0], metricsMNB['confusion'][1][1]))\n",
    "print('\\tAcurácia - {:.4f}'.format(metricsMNB['accuracy']))\n",
    "print('\\tPrecisão - {:.4f}'.format(metricsMNB['precision']))\n",
    "print('\\tSensibilidade - {:.4f}'.format(metricsMNB['recall']))\n",
    "\n",
    "print('\\nDecision Tree:')\n",
    "print('\\tMatriz de Confusão - VN:{}, FP:{}, FN:{}, VP:{}'.format(metricsDT['confusion'][0][0], metricsDT['confusion'][0][1], metricsDT['confusion'][1][0], metricsDT['confusion'][1][1]))\n",
    "print('\\tAcurácia - {:.4f}'.format(metricsDT['accuracy']))\n",
    "print('\\tPrecisão - {:.4f}'.format(metricsDT['precision']))\n",
    "print('\\tSensibilidade - {:.4f}'.format(metricsDT['recall']))\n",
    "\n",
    "print('\\nFloresta Aleatória:')\n",
    "print('\\tMatriz de Confusão - VN:{}, FP:{}, FN:{}, VP:{}'.format(metricsRF['confusion'][0][0], metricsRF['confusion'][0][1], metricsRF['confusion'][1][0], metricsRF['confusion'][1][1]))\n",
    "print('\\tAcurácia - {:.4f}'.format(metricsRF['accuracy']))\n",
    "print('\\tPrecisão - {:.4f}'.format(metricsRF['precision']))\n",
    "print('\\tSensibilidade - {:.4f}'.format(metricsRF['recall']))\n",
    "\n",
    "print('\\nAumento Adaptativo:')\n",
    "print('\\tMatriz de Confusão - VN:{}, FP:{}, FN:{}, VP:{}'.format(metricsAB['confusion'][0][0], metricsAB['confusion'][0][1], metricsAB['confusion'][1][0], metricsAB['confusion'][1][1]))\n",
    "print('\\tAcurácia - {:.4f}'.format(metricsAB['accuracy']))\n",
    "print('\\tPrecisão - {:.4f}'.format(metricsAB['precision']))\n",
    "print('\\tSensibilidade - {:.4f}'.format(metricsAB['recall']))"
   ]
  }
 ]
}